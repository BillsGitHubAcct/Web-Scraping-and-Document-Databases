{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mission_to_mars.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News\n",
    "* Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragragh Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News title --> NASA Engineers Dream Big with Small Spacecraft\n",
      "News paragraph --> \n",
      "The first CubeSat mission to deep space will launch in May.\n"
     ]
    }
   ],
   "source": [
    "# Usea splinter here since the web page has to be fully rendered (js executed) before we are able to get\n",
    "# the latest title and paragraph \n",
    "with  Browser('chrome', **executable_path, headless=True) as browser:\n",
    "    url = 'https://mars.nasa.gov/news/'\n",
    "    # open web page\n",
    "    browser.visit(url)\n",
    "    # scrape web page using beautiful soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # find latest title and paragraph\n",
    "    news_title = soup.find('div', class_='content_title').a.text\n",
    "    news_p = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "print('News title --> ' + news_title)\n",
    "print('News paragraph --> \\n' + news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "\n",
    "* Visit the url for JPL's Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "\n",
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "* Make sure to find the image url to the full size `.jpg` image.\n",
    "\n",
    "* Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image url --> //www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA14884_ip.jpg\n"
     ]
    }
   ],
   "source": [
    "with  Browser('chrome', **executable_path, headless=True) as browser:\n",
    "    jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    # open web page\n",
    "    browser.visit(jpl_url)\n",
    "    # scrape web page using beautiful soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Get url for image jpg\n",
    "    jpl_url = soup.find('div', class_= 'jpl_logo').a['href']\n",
    "    url_last = soup.find('div', class_='default floating_text_area ms-layer').footer.a['data-fancybox-href']\n",
    "    image_url = jpl_url[:-1] + url_last \n",
    "print('image url --> ' + image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Weather\n",
    "\n",
    "* Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called `mars_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mars_weather = Sol 2026 (April 18, 2018), Sunny, high -6C/21F, low -73C/-99F, pressure at 7.19 hPa, daylight 05:26-17:21\n"
     ]
    }
   ],
   "source": [
    "# Scrape Weather from Twitter Page using Soup\n",
    "twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "response = requests.get(twitter_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "mars_weather = soup.find('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text\n",
    "print('mars_weather = ' + mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts\n",
    "\n",
    "* Visit the Mars Facts webpage [here](http://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "* Use Pandas to convert the data to a HTML table string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th>description</th>\\n      <th>value</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Equatorial Diameter:</td>\\n      <td>6,792 km</td>\\n    </tr>\\n    <tr>\\n      <td>Polar Diameter:</td>\\n      <td>6,752 km</td>\\n    </tr>\\n    <tr>\\n      <td>Mass:</td>\\n      <td>6.42 x 10^23 kg (10.7% Earth)</td>\\n    </tr>\\n    <tr>\\n      <td>Moons:</td>\\n      <td>2 (Phobos &amp; Deimos)</td>\\n    </tr>\\n    <tr>\\n      <td>Orbit Distance:</td>\\n      <td>227,943,824 km (1.52 AU)</td>\\n    </tr>\\n    <tr>\\n      <td>Orbit Period:</td>\\n      <td>687 days (1.9 years)</td>\\n    </tr>\\n    <tr>\\n      <td>Surface Temperature:</td>\\n      <td>-153 to 20 Â°C</td>\\n    </tr>\\n    <tr>\\n      <td>First Record:</td>\\n      <td>2nd millennium BC</td>\\n    </tr>\\n    <tr>\\n      <td>Recorded By:</td>\\n      <td>Egyptian astronomers</td>\\n    </tr>\\n  </tbody>\\n</table>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://space-facts.com/mars/'\n",
    "# Use pandas to read all tables from the web page into an array of dataframes\n",
    "df = pd.read_html(url)\n",
    "df[0].columns = ['description','value']\n",
    "# Assign a dataframe with an html table\n",
    "mars_html_table = df[0].to_html(index=False)\n",
    "mars_html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Mars Hemisperes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with  Browser('chrome', **executable_path, headless=False) as browser:\n",
    "    astro_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    # open web page\n",
    "    browser.visit(astro_url)\n",
    "    # scrape web page using beautiful soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    results = soup.find_all('div', class_='item')\n",
    "    mars_images = []\n",
    "    # Read through items to get images from second page\n",
    "    for first_page in results:\n",
    "        # Get image title from first page\n",
    "        title_text = first_page.h3.text\n",
    "        # Click on title's link on first page and go to second page\n",
    "        browser.click_link_by_partial_text(first_page.h3.text)\n",
    "        # Wait for second page processing\n",
    "        time.sleep(2)\n",
    "        # Get second page's url\n",
    "        page2_url = browser.url\n",
    "        # Visit second page so parsing can begin\n",
    "        browser.visit(page2_url)\n",
    "        # Get second page's html for parsing\n",
    "        html2 = browser.html\n",
    "        # Begin parse of second page using beautifulsoup\n",
    "        soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "        # Get Sample image's url on second page\n",
    "        img_url = soup2.find('a',href=True, text='Sample')['href']\n",
    "        # Save title and image url to list of dicts\n",
    "        mars_images.append({\"title\" : title_text,\"img_url\" : img_url})\n",
    "        # Go back to first page\n",
    "        browser.back()\n",
    "    print(mars_images)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape_mars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mars_scrape():\n",
    "\t\"\"\"\n",
    "\t\tFrom Mars related sites scrape content for eventual \n",
    "\t\tinsert into mongodb and display on web page\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport requests\n",
    "\timport time\n",
    "\tfrom bs4 import BeautifulSoup\n",
    "\tfrom splinter import Browser\n",
    "\texecutable_path = {'executable_path': 'chromedriver.exe'}\n",
    "\treturn_list = []\n",
    "\t# ### NASA Mars News\n",
    "\twith  Browser('chrome', **executable_path, headless=True) as browser:\n",
    "\t\turl = 'https://mars.nasa.gov/news/'\n",
    "\t\t# open web page\n",
    "\t\tbrowser.visit(url)\n",
    "\t\t# scrape web page using beautiful soup\n",
    "\t\thtml = browser.html\n",
    "\t\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\t\t# find latest title and paragraph\n",
    "\t\tnews_title = soup.find('div', class_='content_title').a.text\n",
    "\t\treturn_list.append(news_title)\n",
    "\t\ttime.sleep(3)\n",
    "\t\tnews_p = soup.find('div', class_='article_teaser_body').text\n",
    "\t\treturn_list.append(news_p)\n",
    "\t\ttime.sleep(3)\n",
    "\t\t\n",
    "\tprint('News title --> ' + news_title)\n",
    "\tprint('News paragraph --> \\n' + news_p)\n",
    "\n",
    "\t# ### JPL Mars Space Images - Featured Image\n",
    "\twith  Browser('chrome', **executable_path, headless=True) as browser:\n",
    "\t\tjpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\t\t# open web page\n",
    "\t\tbrowser.visit(jpl_url)\n",
    "\t\t# scrape web page using beautiful soup\n",
    "\t\thtml = browser.html\n",
    "\t\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\t\t# Get url for image jpg\n",
    "\t\tjpl_url = soup.find('div', class_= 'jpl_logo').a['href']\n",
    "\t\turl_last = soup.find('div', class_='default floating_text_area ms-layer').footer.a['data-fancybox-href']\n",
    "\t\timage_url = jpl_url[:-1] + url_last\n",
    "\t\treturn_list.append(image_url)\n",
    "\tprint('image url --> ' + image_url)\n",
    "\t\n",
    "\n",
    "\t# ### Mars Weather\n",
    "\n",
    "\ttwitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\tresponse = requests.get(twitter_url)\n",
    "\tsoup = BeautifulSoup(response.text, 'html.parser')\n",
    "\tmars_weather = soup.find('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text\n",
    "\tprint('mars_weather = ' + mars_weather)\n",
    "\treturn_list.append([mars_weather])\n",
    "\t# ### Mars Facts\n",
    "\n",
    "\turl = 'http://space-facts.com/mars/'\n",
    "\t# Use pandas to read all tables from the web page into an array of dataframes\n",
    "\tdf = pd.read_html(url)\n",
    "\tdf[0].columns = ['description','value']\n",
    "\t# Assign a dataframe with an html table\n",
    "\tmars_html_table = df[0].to_html(index=False)\n",
    "\tprint(mars_html_table)\n",
    "\treturn_list.append(mars_html_table)\n",
    "\t# ### Mars Hemisperes\n",
    "\n",
    "\twith  Browser('chrome', **executable_path, headless=True) as browser:\n",
    "\t\tastro_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\t\t# open web page\n",
    "\t\tbrowser.visit(astro_url)\n",
    "\t\t# scrape web page using beautiful soup\n",
    "\t\thtml = browser.html\n",
    "\t\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\t\tresults = soup.find_all('div', class_='item')\n",
    "\t\tmars_images = []\n",
    "\t\t# Read through items to get images from second page\n",
    "\t\tcount = 1\n",
    "\t\tfor first_page in results:\n",
    "\t\t\t# Get image title from first page\n",
    "\t\t\ttitle_text = first_page.h3.text\n",
    "\t\t\t# Click on title's link on first page and go to second page\n",
    "\t\t\tbrowser.click_link_by_partial_text(first_page.h3.text)\n",
    "\t\t\t# Wait for second page processing\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\t# Get second page's url\n",
    "\t\t\tpage2_url = browser.url\n",
    "\t\t\t# Visit second page so parsing can begin\n",
    "\t\t\tbrowser.visit(page2_url)\n",
    "\t\t\t# Get second page's html for parsing\n",
    "\t\t\thtml2 = browser.html\n",
    "\t\t\t# Begin parse of second page using beautifulsoup\n",
    "\t\t\tsoup2 = BeautifulSoup(html2, 'html.parser')\n",
    "\t\t\t# Get Sample image's url on second page\n",
    "\t\t\timg_url = soup2.find('a',href=True, text='Sample')['href']\n",
    "\t\t\t# Save title and image url to list of dicts\n",
    "\t\t\ttitle_desc = \"title\" + str(count)\n",
    "\t\t\timg_desc = \"img_url\" + str(count)\t\t\t\n",
    "\t\t\tmars_images.append({\"title\" : title_text,\"img_url\" : img_url})\n",
    "\t\t\t# Go back to first page\n",
    "\t\t\tbrowser.back()\n",
    "\t\treturn_list.append(mars_images)\n",
    "\t# print('mars_images = ')\n",
    "\t# print(mars_images[0])\n",
    "\t# print(mars_images[1])\n",
    "\t# print(mars_images[2])\n",
    "\t# print(mars_images[3])\n",
    "\ttitle_text1 = mars_images[0][\"title\"]\n",
    "\ttitle_text2 = mars_images[1][\"title\"]\n",
    "\ttitle_text3 = mars_images[2][\"title\"]\n",
    "\ttitle_text4 = mars_images[3][\"title\"]\n",
    "\timage_url1 = mars_images[0][\"img_url\"]\n",
    "\timage_url2 = mars_images[1][\"img_url\"]\n",
    "\timage_url3 = mars_images[2][\"img_url\"]\n",
    "\timage_url4 = mars_images[3][\"img_url\"]\n",
    "\treturn {\"news_title\": news_title, \n",
    "\t\t\t\"news_p\": news_p, \n",
    "\t\t\t\"images_url\": image_url,\n",
    "\t\t\t\"mars_weather\": mars_weather,\n",
    "\t\t\t\"mars_html_table\": mars_html_table,\n",
    "\t\t\t\"title1\": title_text1,\n",
    "\t\t\t\"img_url1\": image_url1,\n",
    "\t\t\t\"title2\": title_text2,\n",
    "\t\t\t\"img_url2\": image_url2,\n",
    "\t\t\t\"title3\": title_text3,\n",
    "\t\t\t\"img_url3\": image_url3,\n",
    "\t\t\t\"title4\": title_text4,\n",
    "\t\t\t\"img_url4\": image_url4\t\t\n",
    "\t\t\t} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Driver for Scraping Mars Web pages, saving scraped data to Mongodb database\n",
    "#\n",
    "from flask import Flask, render_template, redirect\n",
    "from flask_pymongo import PyMongo\n",
    "from scrape_mars import mars_scrape\n",
    "\n",
    "## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "\n",
    "app = Flask(__name__, static_url_path='/static') \n",
    "\n",
    "\n",
    "mongo = PyMongo(app)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "\t\"\"\"\n",
    "\t\tRead Data from Mongodb and then Render Screen with the data read\n",
    "\t\"\"\"\n",
    "\tmars = mongo.db.mars.find_one()\n",
    "\treturn render_template('index.html', mars=mars)\n",
    "\n",
    "\n",
    "@app.route('/scrape')\n",
    "def scrape():\n",
    "\t\"\"\"\n",
    "\t\tCreate Mars Mongodb, call scrape routine and then upsert scraped data to Mongodb\n",
    "\t\"\"\"\n",
    "\tmars = mongo.db.mars\n",
    "\tdata = mars_scrape()\n",
    "\t# print(data['news_title'])\n",
    "\t# print(data['news_p'])\n",
    "\t# print(data['images_url'])\n",
    "\t# print(data['mars_weather'])\n",
    "\t# print(data['mars_html_table'])\n",
    "\t# print(data['mars_images'])\n",
    "\tmars.update(\n",
    "        {},\n",
    "        data,\n",
    "        upsert=True\n",
    "\t\n",
    "    )\n",
    "\treturn redirect(\"http://localhost:5000/\", code=302)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
